---
layout: layout
title: "Research Overview"
---

<center>
  Here's my <a href="https://scholar.google.es/citations?user=Hy-Z3PoAAAAJ&hl=en">Google Scholar</a>.
</center>

<!-- <h2 id='preprints' class="page-heading">Preprints</h2> -->


<div class="divider"></div>


<h2 id='publications' class="page-heading"> Selected Publications</h2>

<div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/icecet.jpg">
<!--       <img style="margin-top:0em" src="/images/research/image_reincarnation.png"> -->
    <table>
      <tr>
        <td><a href="https://doi.org/10.1109/ICECET55527.2022.9872566">DOI</a></td>

      </tr>
    </table>
  </div>

  <div class="six columns">

    <b> Vision Transformer based knowledge distillation for fasteners defect detection </b>
    <p> Vignesh Sampathm, Iñaki Maurtua, J.J. Aguilar, <b>Ander Iriondo</b>, Iker Lluvia, Andoni Rivera <br />
    2022 International Conference on Electrical, Computer and Energy Technologies (ICECET) </p>
  <p> Computer vision based visual inspection systems are gaining enormous importance for manufacturing quality control in recent years due to the advent of Convolutional neural networks (CNN) and transformer-based (vision) models. CNN based models attempt to extract global features by gradually increasing the receptive field, while long-range dependencies are ignored. Therefore, CNN recognizes objects based on the texture instead of the shape. Transformer models, on the other hand, enable modeling long range dependencies using self-attention mechanism. But learning ability of spatial information inside each patch is limited, which means it can disregard a significant spatial local pattern, such as texture. In this work, we propose to combine transformer-based and CNN-based models to take advantage of the strengths of both methods. To meet inference time constraints of real time defect classification tasks, we exploit knowledge distillation method (KD) using softened logits of ensemble model as supervision to train a lightweight CNN model (Resnet18). The study showed that the proposed vision transformer-based KD approach overcome the requirements of limited computational resources and can be deployed on low-power and resource limited devices. The experimental results also showed that proposed framework outperforms in terms of mean accuracy on the test datasets compared to stand-alone CNN methods.
  </p>
  </div>
</div>

<div class="divider"></div>

<div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/sensors.png">
<!--       <img style="margin-top:0em" src="/images/research/image_reincarnation.png"> -->
    <table>
      <tr>
        <td><a href="https://doi.org/10.3390/s21030816">DOI</a></td>

      </tr>
    </table>
  </div>

  <div class="six columns">

    <b> Affordance-Based Grasping Point Detection Using Graph Convolutional Networks for Industrial Bin-Picking Applications </b>
    <p> <b>Ander Iriondo</b>, Elena Lazkano, Ander Ansuategi <br />
    MDPI Sensors </p>
  <p> Grasping point detection has traditionally been a core robotic and computer vision problem. In recent years, deep learning based methods have been widely used to predict grasping points, and have shown strong generalization capabilities under uncertainty. Particularly, approaches that aim at predicting object affordances without relying on the object identity, have obtained promising results in random bin-picking applications. However, most of them rely on RGB/RGB-D images, and it is not clear up to what extent 3D spatial information is used. Graph Convolutional Networks (GCNs) have been successfully used for object classification and scene segmentation in point clouds, and also to predict grasping points in simple laboratory experimentation. In the present proposal, we adapted the Deep Graph Convolutional Network model with the intuition that learning from n-dimensional point clouds would lead to a performance boost to predict object affordances. To the best of our knowledge, this is the first time that GCNs are applied to predict affordances for suction and gripper end effectors in an industrial bin-picking environment. Additionally, we designed a bin-picking oriented data preprocessing pipeline which contributes to ease the learning process and to create a flexible solution for any bin-picking application. To train our models, we created a highly accurate RGB-D/3D dataset which is openly available on demand. Finally, we benchmarked our method against a 2D Fully Convolutional Network based method, improving the top-1 precision score by 1.8% and 1.7% for suction and gripper respectively.
  </p>
  </div>
</div>

<div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/applsci.jpg">
<!--       <img style="margin-top:0em" src="/images/research/image_reincarnation.png"> -->
      <table>
        <tr>
          <td><a href="https://doi.org/10.3390/app9020348">DOI</a></td>

        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Pick and Place Operations in Logistics Using a Mobile Manipulator Controlled with Deep Reinforcement Learning </b>
      <p> <b>Ander Iriondo</b>, Elena Lazkano, Loreto Susperregi, Julen Urain, Ane Fernandez, Jorge Molina <br />
      MDPI Applied Sciences </p>
    <p> Programming robots to perform complex tasks is a very expensive job. Traditional path planning and control are able to generate point to point collision free trajectories, but when the tasks to be performed are complex, traditional planning and control become complex tasks. This study focused on robotic operations in logistics, specifically, on picking objects in unstructured areas using a mobile manipulator configuration. The mobile manipulator has to be able to place its base in a correct place so the arm is able to plan a trajectory up to an object in a table. A deep reinforcement learning (DRL) approach was selected to solve this type of complex control tasks. Using the arm planner’s feedback, a controller for the robot base is learned, which guides the platform to such a place where the arm is able to plan a trajectory up to the object. In addition the performance of two DRL algorithms ((Deep Deterministic Policy Gradient (DDPG)) and (Proximal Policy Optimisation (PPO)) is compared within the context of a concrete robotic task.
    </p>
    </div>
  </div>

<div class="divider"></div>

<!--  <div class="row">
    <div class="six columns">
     <iframe width="500" height="290" src="https://www.youtube.com/embed/XSY9JwqD-bw" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      <table>
        <tr>
          <td><a href="http://agarwl.github.io/rliable">Website</a></td>
          <td><a href="https://twitter.com/agarwl_/status/1432800830621687817?s=20">#TweePrint</a></td>
          <td><a href="https://arxiv.org/abs/2108.13264">ArXiv</a></td>
          <td><a href="https://github.com/google-research/rliable">Library</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Deep RL at the Edge of the Statistical Precipice</b>
      <p> <b>Rishabh Agarwal</b>, Max Schwarzer, Pablo Samuel Castro, Aaron Courville, Marc G. Bellemare <br />
      NeurIPS 2021 (<span style='color:red'>Outstanding Paper Award</span>) </p>
      </p>
    <p> Our findings call for a change in how we evaluate performance on deep RL benchmarks, for which we present more reliable protocols and an
      <a href="https://github.com/google-research/rliable"> open-source library </a>, easily applicable with *even a handful of runs*, to prevent unreliable results
        from stagnating the field.
    </p>
    </div>
  </div>

  <div class="divider"></div>



  <div class="row">
    <div class="six columns">
      <div id="presentation-embed-38953630"></div>
      <script src='https://slideslive.com/embed_presentation.js'></script>
      <script>
          embed = new SlidesLiveEmbed('presentation-embed-38953630', {
              presentationId: '38953630',
              autoPlay: false, // change to true to autoplay the embedded presentation
              verticalEnabled: true
          });
      </script>
      <table>
        <tr>
          <td><a href="https://agarwl.github.io/pse">Website</a></td>
          <td><a href="https://arxiv.org/abs/2101.05265">Paper</a></td>
          <td><a href="https://slideslive.com/38942373">Talk</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning </b>
      <p> <b>Rishabh Agarwal</b>, Marlos C. Machado, Pablo Samuel Castro, Marc G. Bellemare <br />
      ICLR 2021 (<span style='color:red'>Spotlight</span>) </p>
    <p> To improve generalization, we learn representations, via a contrastive loss, that puts states together with similar long-term optimal behavior. This is orthogonal to existing
    approaches such as data augmentation. An earlier version was accepted as an oral presentation at NeurIPS 2020 Workshop on Biological and Artificial RL. </p>
    </div>
  </div>

  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/iup.png">
      <table>
        <tr>
          <td><a href="https://agarwl.github.io/iup">Website</a></td>
          <td><a href="https://arxiv.org/abs/2010.14498">ArXiv</a></td>
          <td><a href="https://www.youtube.com/watch?v=dgnpGl2iNw8">Talk</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning </b>
      <p> Aviral Kumar*, <b>Rishabh Agarwal*</b>, Dibya Ghosh, Sergey Levine <br /> ICLR 2021 </p>
    <p> We identify an implicit under-parameterization phenomenon in value-based deep RL methods that use bootstrapping: when value functions,
      approximated using deep neural networks, are trained with gradient descent using iterated regression onto target values generated by
      previous instances of the value network, more gradient updates decrease the expressivity of the current value network. </p>
    </div>
  </div>
  <div class="divider"></div>

  <div class="row">
    <div class="six columns">
      <img style="margin-top:0em" src="/images/research/rl_unplugged.png">
      <table>
        <tr>
          <td><a href="https://arxiv.org/abs/2006.13888">ArXiv</a></td>
          <td><a href="https://github.com/deepmind/deepmind-research/tree/master/rl_unplugged">Code</a></td>
        </tr>
      </table>
    </div>

    <div class="six columns">

      <b> RL Unplugged: Benchmarks for Offline Reinforcement Learning </b>
      <p> Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo, Konrad Zolna, <b>Rishabh Agarwal</b>,
        Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Ofir Nachum,
        George Tucker, Nicolas Heess, Nando de Freitas <br /> NeurIPS 2020 </p>
    <p> We propose a benchmark called RL Unplugged to evaluate and compare offline RL methods on a diverse range of domains. We provide detailed evaluation
      protocols for each domain and provide an extensive analysis of existing methods using these protocols. We hope that our suite of benchmarks will
      increase the reproducibility in offline RL and make it possible to study challenging tasks with a limited computational budget, thus making RL research
      both more systematic and more accessible across the community.
    </p>
    </div>
  </div>

 <div class="divider"></div>


 <div class="row">
  <div class="six columns">
    <img style="margin-top:0em" src="/images/research/OFFLINE_RL.gif">
    <table>
      <tr>
        <td><a href="https://arxiv.org/abs/1907.04543">ArXiv</a></td>
        <td><a href="https://offline-rl.github.io">Website</a></td>
        <td><a href="https://slideslive.com/38922701/contributed-talk-striving-for-simplicity-in-offpolicy-deep-reinforcement-learning">Talk</a></td>
      </tr>
    </table>
  </div>

    <div class="six columns">

      <b> An Optimistic Perspective on Offline Reinforcement Learning</b>
      <p> <b>Rishabh Agarwal</b>, Dale Schuurmans, Mohammad Norouzi <br />
      ICML 2020 (<span style='color:red'>Talk</span>)</p>
    <p> This paper popularized offline RL and showed that standard off-policy algorithms perform quite well in the fully
      off-policy / offline deep RL setting with large and diverse datasets. A previous version was titled "Striving for Simplicity in Off-Policy Deep Reinforcement Learning"
      and presented as a contributed talk at NeurIPS 2019 DRL workshop.
    </p>
    </div>
 </div>
-->
